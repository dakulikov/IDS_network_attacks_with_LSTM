{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import socket, struct\n",
    "from sklearn.preprocessing import OneHotEncoder, normalize, MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import keras_metrics as km\n",
    "from keras.layers import merge, Input, Dropout, Concatenate,Activation\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kulik\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Kulik\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700001 700001 700001\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./UNSW-NB15_1.csv', header=None)\n",
    "data1a = numpy.asarray(data)\n",
    "data1a = numpy.delete(data1a, 47, 1)\n",
    "data2 = pd.read_csv('./UNSW-NB15_2.csv', header=None)\n",
    "data2a = numpy.asarray(data2)\n",
    "data2a = numpy.delete(data2a, 47, 1)\n",
    "data3 = pd.read_csv('./UNSW-NB15_3.csv', header=None)\n",
    "data3a = numpy.asarray(data3)\n",
    "data3a = numpy.delete(data3a, 47, 1)\n",
    "#data4 = pd.read_csv('./UNSW-NB15_4.csv', header=None)\n",
    "#data4a = numpy.asarray(data4)\n",
    "#data4a = numpy.delete(data4a, 47, 1)\n",
    "print(len(data1a),len(data2a),len(data3a))\n",
    "#data1 = numpy.append(data1a,data2a)\n",
    "#data1 = numpy.append(data1, data3a)\n",
    "#data1 = numpy.append(data1, data4a)\n",
    "#print(len(data1))\n",
    "#print(data1[0])\n",
    "#data1=numpy.reshape(data1,(2540047,48))\n",
    "#print(len(data1))\n",
    "data = []\n",
    "data.append(data1a)\n",
    "data.append(data2a)\n",
    "data.append(data3a)\n",
    "data = numpy.reshape(data,(2100003,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SkMetrics(Callback):\n",
    "    def __init__(self,batchsize):\n",
    "        self.batchsize = batchsize\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.confusion = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1s = []\n",
    "        self.kappa = []        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        score = numpy.asarray(self.model.predict(self.validation_data[0],batch_size = self.batchsize))\n",
    "        predict = numpy.round(numpy.asarray(self.model.predict(self.validation_data[0],batch_size = self.batchsize)))\n",
    "        targ = self.validation_data[1]\n",
    "        targ = targ.flatten()\n",
    "        predict = predict.flatten()\n",
    "        self.confusion.append(confusion_matrix(targ, predict, labels=[0,1]))\n",
    "        self.precision.append(precision_score(targ, predict))\n",
    "        self.recall.append(recall_score(targ, predict))\n",
    "        self.f1s.append(f1_score(targ, predict))\n",
    "        self.kappa.append(cohen_kappa_score(targ, predict))\n",
    "\n",
    "def getModel(neurons, layers, inputshape, batchsize):\n",
    "    vec = Input(batch_shape=(batchsize, inputshape[0],inputshape[1]))\n",
    "    L1 = LSTM(neurons, return_sequences=True, stateful=True)(vec)\n",
    "    L1_d = Dropout(0.2)(L1)\n",
    "    input2 = Concatenate()([vec,L1_d])\n",
    "    L2 = LSTM(neurons, return_sequences=True, stateful=True)(input2)\n",
    "    L2_d = Dropout(0.2)(L2)\n",
    "    #input_d = Concatenate()([L1_d,L2_d])\n",
    "    input3 = Concatenate()([vec, L2_d])\n",
    "    L3 = LSTM(neurons, return_sequences=True, stateful=True)(input3)\n",
    "    L3_d = Dropout(0.2)(L3)\n",
    "    input_d = Concatenate()([L1_d,L2_d,L3_d])\n",
    "    dense = Dense(1, activation = 'sigmoid')(input_d)\n",
    "    model = Model(input=vec, output=dense)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iptoint(ip):\n",
    "    return struct.unpack(\"!I\",(socket.inet_aton(ip)))[0] \n",
    "\n",
    "def parse(data):\n",
    "    protocols = dict()\n",
    "    protocolsNum = 1\n",
    "    services  = dict()\n",
    "    servicesNum = 1\n",
    "    states = dict()\n",
    "    statesNum = 1\n",
    "    numMalicious = 0\n",
    "    numNormal = 0\n",
    "    x = []\n",
    "    y = []\n",
    "    for dataRaw in data:\n",
    "        dataRaw[0] = iptoint(dataRaw[0])\n",
    "        dataRaw[2] = iptoint(dataRaw[2])\n",
    "        #Source IP\n",
    "        if (type(dataRaw[39]) == str):\n",
    "            if (dataRaw[39] !=  \" \"):\n",
    "                dataRaw[39] = int(dataRaw[39],0)\n",
    "            else:\n",
    "                dataRaw[39] = 0\n",
    "        if (type(dataRaw[1]) == str):\n",
    "            if (dataRaw[1] != \"-\"):\n",
    "                dataRaw[1] = int(dataRaw[1],0)\n",
    "            else:\n",
    "                dataRaw[1] = 0\n",
    "        #Dest IP\n",
    "        if (type(dataRaw[3]) == str):\n",
    "            if (dataRaw[3] != \"-\"):\n",
    "                dataRaw[3] = int(dataRaw[3],0)\n",
    "            else:\n",
    "                dataRaw[3] = 0\n",
    "        if (dataRaw[4] in protocols):\n",
    "            dataRaw[4] = protocols.get(dataRaw[4])\n",
    "        else:\n",
    "            protocols.update({dataRaw[4]:protocolsNum})\n",
    "            dataRaw[4] = protocolsNum\n",
    "            protocolsNum += 1\n",
    "\n",
    "        if (type(dataRaw[4])!= int):\n",
    "            print (dataRaw[4])\n",
    " \n",
    "        if (dataRaw[5] in states):\n",
    "            dataRaw[5] = states.get(dataRaw[5])\n",
    "        else:\n",
    "            states.update({dataRaw[5]:statesNum})\n",
    "            dataRaw[5] = statesNum\n",
    "            statesNum += 1\n",
    "        if (type(dataRaw[5])!=int):\n",
    "            print (dataRaw[5])\n",
    "        if (dataRaw[13] in services):\n",
    "            dataRaw[13] = services.get(dataRaw[13])\n",
    "        else:\n",
    "            services.update({dataRaw[13]:servicesNum})\n",
    "            dataRaw[13] = servicesNum\n",
    "            servicesNum += 1\n",
    "        if (type(dataRaw[13])!=int):\n",
    "            print (dataRaw[13])\n",
    "        if (dataRaw[47] == 1):\n",
    "            numMalicious = numMalicious + 1\n",
    "            y.append(1)\n",
    "        else:\n",
    "            numNormal = numNormal + 1\n",
    "            y.append(0)\n",
    "        i = 0\n",
    "        for data in dataRaw:\n",
    "            if (type(data) == str):\n",
    "                print(dataRaw,data,i)\n",
    "            i = i+1\n",
    "        x.append(dataRaw)\n",
    "    print(\"normal = \",numNormal)\n",
    "    print(\"Malicious = \", numMalicious)\n",
    "    print(protocols)\n",
    "    print(states)\n",
    "    print(services)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def optimizeParameters(data, labels, scorefunc):\n",
    "    # Выбираем k лучших признаков\n",
    "    selecter = SelectKBest(score_func=scorefunc, k=10)\n",
    "    selecter.fit(data, labels)\n",
    "    \n",
    "    # Какие именно выбрали\n",
    "    params = []\n",
    "    index = []\n",
    "    params = selecter.get_support()\n",
    "    count = 0\n",
    "    \n",
    "    for param in params :\n",
    "        if param == True :\n",
    "            index.append(count)\n",
    "        count += 1\n",
    "    \n",
    "\n",
    "    index_file = open(\"index.txt\" , \"w\")\n",
    "    for i in index : \n",
    "        index_file.write(str(i) + \" \")\n",
    "    index_file.close()\n",
    "\n",
    "    return selecter.transform(data)\n",
    "\n",
    "def drawGraphic(dataset, labels, normalCount, malwareCount):\n",
    "\n",
    "    parametersCount = len(dataset[0])\n",
    "\n",
    "    normalCountList = [0] * parametersCount\n",
    "    malwareCountList = [0] * parametersCount\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        if(i < normalCount):\n",
    "            for j in range(0, parametersCount):\n",
    "                normalCountList[j] += dataset[i][j]\n",
    "        else:\n",
    "            for j in range(0, parametersCount):\n",
    "                malwareCountList[j] += dataset[i][j]\n",
    "\n",
    "    for i in range(0, parametersCount):\n",
    "        normalCountList[i] = normalCountList[i]\n",
    "        malwareCountList[i] = malwareCountList[i] \n",
    "\n",
    "    ts = pd.DataFrame({'normal': normalCountList,\n",
    "                       'malware': malwareCountList})\n",
    "    ts.plot()\n",
    "    plt.show()\n",
    "    return\n",
    "def ROC(ROC_MODEL , ROC_X, ROC_Y):\n",
    "    probs = ROC_MODEL.predict(ROC_X)\n",
    "    ROC_Y = ROC_Y.flatten()\n",
    "    probs = probs.flatten()\n",
    "    FPr, TPr, threshold = metrics.roc_curve(ROC_Y, probs)\n",
    "    roc_auc = metrics.auc(FPr, TPr)\n",
    "    lw = 2\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(FPr, TPr, lw = lw , label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "def CrossValidation(lenOfBlock , X1,X2,X3, Y1, Y2, Y3, neurons, layers, inputshape, epochsIn, batchsize):\n",
    "    cv = KFold(n_splits = lenOfBlock, random_state = None, shuffle = False)\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    F1 = 0\n",
    "    Educate = 0.0\n",
    "    Test = 0.0\n",
    "    print(\"Neurons: \", neurons)\n",
    "    print(\"Layers: \", layers)\n",
    "    numblock=0\n",
    "    for train_index, test_index in cv.split(X1):\n",
    "        clf = getModel(neurons, layers, inputshape, batchsize)\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        print(\"Block: \", numblock)\n",
    "        numblock +=1\n",
    "        # Обучение\n",
    "        #sample_weights = []\n",
    "        #for i in train_index:\n",
    "        #    flag = 0\n",
    "        #    for yres in Y[i]:\n",
    "        #        if yres==1:\n",
    "        #            flag = 1\n",
    "        #            break\n",
    "        #    if flag == 1:\n",
    "        #        sample_weights.append(1)\n",
    "        #    else:\n",
    "        #        sample_weights.append(1)\n",
    "        start_time = time.time()\n",
    "        skmetrics = SkMetrics(batchsize)\n",
    "        for i in range (epochsIn):\n",
    "            #history = clf.fit(X1[train_index], Y1[train_index], callbacks = [skmetrics],epochs = 1, validation_data = (X[test_index],Y[test_index]), sample_weight= numpy.asarray(sample_weights), batch_size = batchsize)\n",
    "            #history = clf.fit(X2[train_index], Y2[train_index], callbacks = [skmetrics],epochs = 1, validation_data = (X[test_index],Y[test_index]), sample_weight= numpy.asarray(sample_weights), batch_size = batchsize)\n",
    "            #history = clf.fit(X3[train_index], Y3[train_index], callbacks = [skmetrics],epochs = 1, validation_data = (X[test_index],Y[test_index]), sample_weight= numpy.asarray(sample_weights), batch_size = batchsize)\n",
    "            history = clf.fit(X1[train_index], Y1[train_index], callbacks = [skmetrics],epochs = 1, validation_data = (X1[test_index],Y1[test_index]), batch_size = batchsize)\n",
    "            history = clf.fit(X2[train_index], Y2[train_index], callbacks = [skmetrics],epochs = 1, validation_data = (X2[test_index],Y2[test_index]), batch_size = batchsize)\n",
    "            history = clf.fit(X3[train_index], Y3[train_index], callbacks = [skmetrics],epochs = 1, validation_data = (X3[test_index],Y3[test_index]), batch_size = batchsize)\n",
    "        for k in skmetrics.confusion:\n",
    "            print(k)\n",
    "        education_time = time.time() - start_time\n",
    "        try:\n",
    "            # Тестирование\n",
    "            start_time = time.time()\n",
    "            proba1 = numpy.round(numpy.asarray(clf.predict(X1[test_index], batch_size = batchsize)))\n",
    "            proba2 = numpy.round(numpy.asarray(clf.predict(X2[test_index], batch_size = batchsize)))\n",
    "            proba3 = numpy.round(numpy.asarray(clf.predict(X3[test_index], batch_size = batchsize)))\n",
    "\n",
    "            test_time = time.time() - start_time\n",
    "            \n",
    "            # Добавляем значения\n",
    "            Educate += education_time\n",
    "            Test += test_time\n",
    "            y1 = Y1[test_index].flatten()\n",
    "            ROC(clf , X1[test_index], Y1[test_index])\n",
    "            ROC(clf , X2[test_index], Y2[test_index])\n",
    "            ROC(clf , X3[test_index], Y3[test_index])\n",
    "            y2 = Y2[test_index].flatten()\n",
    "            y3 = Y3[test_index].flatten()\n",
    "            proba1 = proba1.flatten()\n",
    "            proba2 = proba2.flatten()\n",
    "            proba3 = proba3.flatten()\n",
    "            cn1 = confusion_matrix(y1, proba1, labels=[0,1])\n",
    "            cn2 = confusion_matrix(y2, proba2, labels=[0,1])\n",
    "            cn3 = confusion_matrix(y3, proba3, labels=[0,1])\n",
    "            print(cn1)\n",
    "            print(cn2)\n",
    "            print(cn3)\n",
    "            tn1, fp1, fn1, tp1 = cn1.ravel()\n",
    "            tn2, fp2, fn2, tp2 = cn2.ravel()\n",
    "            tn3, fp3, fn3, tp3 = cn3.ravel()\n",
    "        except ZeroDivisionError:\n",
    "            print(\"ZeroDivisionError\")\n",
    "        TN += tn1 + tn2+ tn3\n",
    "        FP += fp1 +fp2 +fp3\n",
    "        FN += fn1 +fn2 +fn3\n",
    "        TP += tp1 + tp2 +tp3\n",
    "        F1 += (f1_score(y1, proba1 , average='binary'))\n",
    "        F1 += (f1_score(y2, proba2 , average='binary'))\n",
    "        F1 += (f1_score(y3, proba3 , average='binary'))\n",
    "        \n",
    "    summ = TN + FP + FN + TP\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"Block len \" + str(lenOfBlock))\n",
    "    print(\"True positive \" + str(TP / summ), TP)\n",
    "    print(\"True negative \" + str(TN / summ), TN)\n",
    "    print(\"False positive \" + str(FP / summ), FP)\n",
    "    print(\"False negative \" + str(FN / summ), FN)\n",
    "   \n",
    "    print(\"Accuracy \" + str((TP + TN)/(summ)))\n",
    "    print(\"Precision \" + str(TP / (TP + FP))) # Precision\n",
    "    print(\"Recall \" + str(TP / (TP + FN))) # Recall\n",
    "\n",
    "    print(\"F1 score \" + str(F1 / lenOfBlock)) # F1 score\n",
    "    print(\"Educate time \" + str(Educate / lenOfBlock)) # Educate time\n",
    "    print(\"Test time \" + str(Test / lenOfBlock)) # Test time\n",
    "\n",
    "    print(\"\\n\\n\\n\")    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal =  1867614\n",
      "Malicious =  232389\n",
      "{'udp': 1, 'arp': 2, 'tcp': 3, 'ospf': 4, 'icmp': 5, 'igmp': 6, 'sctp': 7, 'udt': 8, 'sep': 9, 'sun-nd': 10, 'swipe': 11, 'mobile': 12, 'pim': 13, 'rtp': 14, 'ipnip': 15, 'ip': 16, 'ggp': 17, 'st2': 18, 'egp': 19, 'cbt': 20, 'emcon': 21, 'nvp': 22, 'igp': 23, 'xnet': 24, 'argus': 25, 'bbn-rcc': 26, 'chaos': 27, 'pup': 28, 'hmp': 29, 'mux': 30, 'dcn': 31, 'prm': 32, 'trunk-1': 33, 'xns-idp': 34, 'trunk-2': 35, 'leaf-1': 36, 'leaf-2': 37, 'irtp': 38, 'rdp': 39, 'iso-tp4': 40, 'netblt': 41, 'mfe-nsp': 42, 'merit-inp': 43, '3pc': 44, 'xtp': 45, 'idpr': 46, 'tp++': 47, 'ddp': 48, 'idpr-cmtp': 49, 'ipv6': 50, 'il': 51, 'idrp': 52, 'ipv6-frag': 53, 'sdrp': 54, 'ipv6-route': 55, 'gre': 56, 'rsvp': 57, 'mhrp': 58, 'bna': 59, 'esp': 60, 'i-nlsp': 61, 'narp': 62, 'ipv6-no': 63, 'tlsp': 64, 'skip': 65, 'ipv6-opts': 66, 'any': 67, 'cftp': 68, 'sat-expak': 69, 'kryptolan': 70, 'rvd': 71, 'ippc': 72, 'sat-mon': 73, 'ipcv': 74, 'visa': 75, 'cpnx': 76, 'cphb': 77, 'wsn': 78, 'pvp': 79, 'br-sat-mon': 80, 'wb-mon': 81, 'wb-expak': 82, 'iso-ip': 83, 'secure-vmtp': 84, 'vmtp': 85, 'vines': 86, 'ttp': 87, 'nsfnet-igp': 88, 'dgp': 89, 'tcf': 90, 'eigrp': 91, 'sprite-rpc': 92, 'larp': 93, 'mtp': 94, 'ax.25': 95, 'ipip': 96, 'micp': 97, 'aes-sp3-d': 98, 'encap': 99, 'etherip': 100, 'pri-enc': 101, 'gmtp': 102, 'pnni': 103, 'ifmp': 104, 'aris': 105, 'qnx': 106, 'a/n': 107, 'scps': 108, 'snp': 109, 'ipcomp': 110, 'compaq-peer': 111, 'ipx-n-ip': 112, 'vrrp': 113, 'zero': 114, 'pgm': 115, 'iatp': 116, 'ddx': 117, 'l2tp': 118, 'srp': 119, 'stp': 120, 'smp': 121, 'uti': 122, 'sm': 123, 'ptp': 124, 'fire': 125, 'crtp': 126, 'isis': 127, 'crudp': 128, 'sccopmce': 129, 'sps': 130, 'pipe': 131, 'iplt': 132, 'unas': 133, 'fc': 134, 'ib': 135}\n",
      "{'CON': 1, 'INT': 2, 'FIN': 3, 'URH': 4, 'REQ': 5, 'ECO': 6, 'RST': 7, 'CLO': 8, 'TXD': 9, 'URN': 10, 'no': 11, 'ACC': 12, 'PAR': 13, 'MAS': 14, 'TST': 15, 'ECR': 16}\n",
      "{'dns': 1, '-': 2, 'http': 3, 'smtp': 4, 'ftp-data': 5, 'ftp': 6, 'ssh': 7, 'pop3': 8, 'snmp': 9, 'ssl': 10, 'irc': 11, 'radius': 12, 'dhcp': 13}\n",
      "700001\n",
      "[1000734720 1390 2511044102 53 1 1 0.0010550000000000002 132 164 31 29 0 0\n",
      " 1 500473.9375 621800.9375 2 2 0 0 0 0 66 82 0 0 0.0 0.0 1421927414\n",
      " 1421927414 0.017 0.013000000000000001 0.0 0.0 0.0 0 0 0 0 0 3 7 1 3 1 1 1\n",
      " 0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "[x, y] = parse(data)\n",
    "#[x2, newy2] = parse(data2a)\n",
    "#[x3, newy3] = parse(data3a)\n",
    "x = numpy.reshape(x,(3, 700001, 48))\n",
    "y = numpy.reshape(y,(3, 700001))\n",
    "newy1 = y[0]\n",
    "newy2 = y[1]\n",
    "newy3 = y[2]\n",
    "\n",
    "x1 = x[0]\n",
    "x2 = x[1]\n",
    "x3 = x[2]\n",
    "\n",
    "newx1 = numpy.delete(numpy.asarray(x1), 47, 1)\n",
    "newx2 = numpy.delete(numpy.asarray(x2), 47, 1)\n",
    "newx3 = numpy.delete(numpy.asarray(x3), 47, 1)\n",
    "\n",
    "print(len(x1))\n",
    "newx1 = newx1.tolist()\n",
    "newx2 = newx2.tolist()\n",
    "newx3 = newx3.tolist()\n",
    "newy1 = newy1.tolist()\n",
    "newy2 = newy2.tolist()\n",
    "newy3 = newy3.tolist()\n",
    "print(x1[0])\n",
    "print(newy1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700001\n",
      "700001\n",
      "700001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(newx1))\n",
    "while (len(newx1)!=700000):\n",
    "    newx1.pop()\n",
    "    newy1.pop()\n",
    "print(len(newx2))\n",
    "while (len(newx2)!=700000):\n",
    "    newx2.pop()\n",
    "    newy2.pop()\n",
    "print(len(newx3))\n",
    "while (len(newx3)!=700000):\n",
    "    newx3.pop()\n",
    "    newy3.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000734720, 1390, 2511044102, 53, 1, 1, 0.0010550000000000002, 132, 164, 31, 29, 0, 0, 1, 500473.9375, 621800.9375, 2, 2, 0, 0, 0, 0, 66, 82, 0, 0, 0.0, 0.0, 1421927414, 1421927414, 0.017, 0.013000000000000001, 0.0, 0.0, 0.0, 0, 0, 0, 0, 0, 3, 7, 1, 3, 1, 1, 1]\n",
      "47\n",
      "700000\n",
      "[2.71179403e-01 2.12100404e-02 6.52407146e-01 9.83321850e-08\n",
      " 0.00000000e+00 0.00000000e+00 1.20068681e-07 9.65096199e-06\n",
      " 1.11904015e-05 1.21568627e-01 1.14173228e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 9.50026411e-05 4.82907536e-03\n",
      " 1.96078431e-04 1.82315406e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.38829787e-02 5.46666667e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.29984191e-03 0.00000000e+00 2.83286497e-07 2.18541314e-07\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.65116279e-02 1.46341463e-01 0.00000000e+00 4.08163265e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "700000\n",
      "[2.71179403e-01 2.12100404e-02 6.52407146e-01 9.83321850e-08\n",
      " 0.00000000e+00 0.00000000e+00 1.20068681e-07 9.65096199e-06\n",
      " 1.11904015e-05 1.21568627e-01 1.14173228e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 9.50026411e-05 4.82907536e-03\n",
      " 1.96078431e-04 1.82315406e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.38829787e-02 5.46666667e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.29984191e-03 0.00000000e+00 2.83286497e-07 2.18541314e-07\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.65116279e-02 1.46341463e-01 0.00000000e+00 4.08163265e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "700000\n",
      "[2.99906766e-01 9.23933776e-02 6.52407146e-01 8.26199741e-01\n",
      " 1.49253731e-02 1.33333333e-01 1.21623356e-03 2.95212226e-04\n",
      " 4.14721961e-03 1.21568627e-01 1.14173228e-01 1.31603685e-03\n",
      " 5.44761213e-03 8.33333333e-02 8.38049408e-05 1.36701961e-01\n",
      " 6.76310351e-03 6.53476130e-03 1.00000000e+00 1.00000000e+00\n",
      " 2.33598597e-01 2.33666088e-01 3.93333333e-02 5.62666667e-01\n",
      " 0.00000000e+00 0.00000000e+00 4.24828203e-05 2.13898705e-04\n",
      " 2.06563080e-05 0.00000000e+00 1.70442572e-05 1.75792989e-05\n",
      " 6.02855249e-04 1.15670814e-03 6.08437507e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.93548387e-01 1.96721311e-01 8.47457627e-02 1.01694915e-01\n",
      " 0.00000000e+00 0.00000000e+00 1.61290323e-02]\n",
      "700000\n",
      "[2.99906767e-01 2.78431373e-01 6.52407146e-01 1.16914626e-01\n",
      " 1.49253731e-02 1.33333333e-01 1.99326780e-03 3.50938545e-04\n",
      " 4.66321632e-03 1.21568627e-01 1.14173228e-01 1.45742244e-03\n",
      " 6.01750547e-03 8.33333333e-02 5.01800716e-05 1.93099493e-01\n",
      " 8.11148087e-03 7.28995808e-03 1.00000000e+00 1.00000000e+00\n",
      " 4.23374661e-01 4.23440770e-01 3.86666667e-02 5.80557444e-01\n",
      " 0.00000000e+00 0.00000000e+00 5.87247448e-05 1.84895854e-04\n",
      " 1.58646220e-03 0.00000000e+00 2.58186315e-05 2.61033917e-05\n",
      " 1.06619498e-04 1.34209067e-04 2.49971802e-05 0.00000000e+00\n",
      " 0.00000000e+00            nan            nan 0.00000000e+00\n",
      " 7.57575758e-02 1.51515152e-02 1.51515152e-02 6.06060606e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.51515152e-02]\n",
      "[2.99906767e-01 2.78431373e-01 6.52407146e-01 1.16914626e-01\n",
      " 1.49253731e-02 1.33333333e-01 1.99326780e-03 3.50938545e-04\n",
      " 4.66321632e-03 1.21568627e-01 1.14173228e-01 1.45742244e-03\n",
      " 6.01750547e-03 8.33333333e-02 5.01800716e-05 1.93099493e-01\n",
      " 8.11148087e-03 7.28995808e-03 1.00000000e+00 1.00000000e+00\n",
      " 4.23374661e-01 4.23440770e-01 3.86666667e-02 5.80557444e-01\n",
      " 0.00000000e+00 0.00000000e+00 5.87247448e-05 1.84895854e-04\n",
      " 1.58646220e-03 0.00000000e+00 2.58186315e-05 2.61033917e-05\n",
      " 1.06619498e-04 1.34209067e-04 2.49971802e-05 0.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 7.57575758e-02 1.51515152e-02 1.51515152e-02 6.06060606e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.51515152e-02]\n"
     ]
    }
   ],
   "source": [
    "print (newx1[0])\n",
    "print (len(newx1[0]))\n",
    "print (len(newx1))\n",
    "min_max_scaler = MinMaxScaler()\n",
    "newx1 = min_max_scaler.fit_transform(newx1)\n",
    "znewx2 = min_max_scaler.fit_transform(newx2)\n",
    "newx3 = min_max_scaler.fit_transform(newx3)\n",
    "print(newx1[0])\n",
    "xnew1= numpy.reshape(newx1, (len(newx1)//5, 5, 47))\n",
    "print(len(newy1))\n",
    "ynew1 = numpy.reshape(newy1,(len(newx1)//5, 5,1))\n",
    "print(xnew1[0][0])\n",
    "xnew2= numpy.reshape(newx2, (len(newx2)//5, 5, 47))\n",
    "print(len(newy2))\n",
    "ynew2 = numpy.reshape(newy2,(len(newx2)//5, 5,1))\n",
    "print(xnew2[0][0])\n",
    "xnew3= numpy.reshape(newx3, (len(newx3)//5, 5, 47))\n",
    "print(len(newy3))\n",
    "ynew3 = numpy.reshape(newy3,(len(newx3)//5, 5,1))\n",
    "print(xnew3[0][0])\n",
    "\n",
    "for i in range(len(xnew1)):\n",
    "    for j in range(len(xnew1[i])):\n",
    "        for k in range(len(xnew1[i][j])):\n",
    "            if numpy.isnan(xnew1[i][j][k]):\n",
    "                xnew1[i][j][k] = 1\n",
    "            if numpy.isnan(xnew2[i][j][k]):\n",
    "                xnew2[i][j][k] = 1\n",
    "            if numpy.isnan(xnew3[i][j][k]):\n",
    "                xnew3[i][j][k] = 1\n",
    "print(xnew3[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#CrossValidation(10 , xnew , ynew, 100, 2, (5,47), 2, 50)\n",
    "numpy.isnan(xnew3[0][0][37])\n",
    "xnew3[0][0][37] = 1\n",
    "print (xnew3[0][0][37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons:  300\n",
      "Layers:  2\n",
      "WARNING:tensorflow:From C:\\Users\\Kulik\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Kulik\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kulik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 28000  28001  28002 ... 139997 139998 139999] TEST: [    0     1     2 ... 27997 27998 27999]\n",
      "Block:  0\n",
      "WARNING:tensorflow:From C:\\Users\\Kulik\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 305s 3ms/step - loss: 0.0130 - acc: 0.9951 - val_loss: 0.0596 - val_acc: 0.9729\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 297s 3ms/step - loss: 0.0582 - acc: 0.9731 - val_loss: 4.5485e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kulik\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:373: RuntimeWarning: invalid value encountered in true_divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 301s 3ms/step - loss: 0.0978 - acc: 0.9536 - val_loss: 0.0977 - val_acc: 0.9560\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 301s 3ms/step - loss: 0.0061 - acc: 0.9968 - val_loss: 0.0718 - val_acc: 0.9714\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 301s 3ms/step - loss: 0.0349 - acc: 0.9848 - val_loss: 5.0510e-05 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 300s 3ms/step - loss: 0.0726 - acc: 0.9691 - val_loss: 0.0852 - val_acc: 0.9612\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 300s 3ms/step - loss: 0.0057 - acc: 0.9972 - val_loss: 0.0561 - val_acc: 0.9751\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 304s 3ms/step - loss: 0.0331 - acc: 0.9861 - val_loss: 7.6092e-07 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 302s 3ms/step - loss: 0.0570 - acc: 0.9758 - val_loss: 0.0482 - val_acc: 0.9849\n",
      "[[101448    655]\n",
      " [  1461  36436]]\n",
      "[[121877    666]\n",
      " [  4193  13264]]\n",
      "[[140000      0]\n",
      " [     0      0]]\n",
      "[[101448    655]\n",
      " [  1461  36436]]\n",
      "TRAIN: [     0      1      2 ... 139997 139998 139999] TEST: [28000 28001 28002 ... 55997 55998 55999]\n",
      "Block:  1\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 312s 3ms/step - loss: 0.0321 - acc: 0.9873 - val_loss: 0.0238 - val_acc: 0.9886\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 307s 3ms/step - loss: 0.0493 - acc: 0.9772 - val_loss: 5.0254e-05 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 305s 3ms/step - loss: 0.1053 - acc: 0.9499 - val_loss: 0.0774 - val_acc: 0.9572\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 305s 3ms/step - loss: 0.0177 - acc: 0.9912 - val_loss: 0.0226 - val_acc: 0.9882\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 306s 3ms/step - loss: 0.0381 - acc: 0.9824 - val_loss: 1.8045e-06 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 306s 3ms/step - loss: 0.0609 - acc: 0.9737 - val_loss: 0.0649 - val_acc: 0.9695\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 306s 3ms/step - loss: 0.0153 - acc: 0.9932 - val_loss: 0.0197 - val_acc: 0.9918\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 310s 3ms/step - loss: 0.0344 - acc: 0.9854 - val_loss: 1.5186e-07 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 309s 3ms/step - loss: 0.0529 - acc: 0.9785 - val_loss: 0.0543 - val_acc: 0.9739\n",
      "[[109861   1319]\n",
      " [  2329  26491]]\n",
      "[[134988    254]\n",
      " [   996   3762]]\n",
      "[[140000      0]\n",
      " [     0      0]]\n",
      "[[109861   1319]\n",
      " [  2329  26491]]\n",
      "TRAIN: [     0      1      2 ... 139997 139998 139999] TEST: [56000 56001 56002 ... 83997 83998 83999]\n",
      "Block:  2\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 318s 3ms/step - loss: 0.0408 - acc: 0.9827 - val_loss: 8.7993e-05 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 310s 3ms/step - loss: 0.0497 - acc: 0.9763 - val_loss: 0.0133 - val_acc: 0.9943\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 311s 3ms/step - loss: 0.0997 - acc: 0.9538 - val_loss: 0.0654 - val_acc: 0.9652\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 310s 3ms/step - loss: 0.0244 - acc: 0.9881 - val_loss: 7.1887e-06 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 310s 3ms/step - loss: 0.0354 - acc: 0.9838 - val_loss: 0.0136 - val_acc: 0.9940\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 310s 3ms/step - loss: 0.0679 - acc: 0.9705 - val_loss: 0.0511 - val_acc: 0.9742\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 311s 3ms/step - loss: 0.0220 - acc: 0.9898 - val_loss: 9.2659e-07 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 311s 3ms/step - loss: 0.0315 - acc: 0.9858 - val_loss: 0.0130 - val_acc: 0.9936\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 310s 3ms/step - loss: 0.0657 - acc: 0.9717 - val_loss: 0.0547 - val_acc: 0.9753\n",
      "[[104164   1877]\n",
      " [  1562  32397]]\n",
      "[[140000      0]\n",
      " [     0      0]]\n",
      "[[137631    349]\n",
      " [   614   1406]]\n",
      "[[104180   1861]\n",
      " [  1564  32395]]\n",
      "TRAIN: [     0      1      2 ... 139997 139998 139999] TEST: [ 84000  84001  84002 ... 111997 111998 111999]\n",
      "Block:  3\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 325s 3ms/step - loss: 0.0337 - acc: 0.9859 - val_loss: 1.0109e-05 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 316s 3ms/step - loss: 0.0187 - acc: 0.9915 - val_loss: 0.0643 - val_acc: 0.9714\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 318s 3ms/step - loss: 0.1244 - acc: 0.9426 - val_loss: 0.0909 - val_acc: 0.9566\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 316s 3ms/step - loss: 0.0231 - acc: 0.9887 - val_loss: 4.0678e-06 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 315s 3ms/step - loss: 0.0182 - acc: 0.9917 - val_loss: 0.0631 - val_acc: 0.9722\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 315s 3ms/step - loss: 0.0493 - acc: 0.9781 - val_loss: 0.0389 - val_acc: 0.9832\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 314s 3ms/step - loss: 0.0182 - acc: 0.9915 - val_loss: 9.8856e-07 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 316s 3ms/step - loss: 0.0159 - acc: 0.9926 - val_loss: 0.0749 - val_acc: 0.9665\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 316s 3ms/step - loss: 0.0470 - acc: 0.9791 - val_loss: 0.0374 - val_acc: 0.9837\n",
      "[[108896   1175]\n",
      " [  1154  28775]]\n",
      "[[140000      0]\n",
      " [     0      0]]\n",
      "[[110107   1367]\n",
      " [  1202  27324]]\n",
      "[[108900   1171]\n",
      " [  1154  28775]]\n",
      "TRAIN: [     0      1      2 ... 111997 111998 111999] TEST: [112000 112001 112002 ... 139997 139998 139999]\n",
      "Block:  4\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 326s 3ms/step - loss: 0.0343 - acc: 0.9854 - val_loss: 5.7335e-06 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 319s 3ms/step - loss: 0.0177 - acc: 0.9925 - val_loss: 0.0638 - val_acc: 0.9730\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 319s 3ms/step - loss: 0.0816 - acc: 0.9656 - val_loss: 0.1560 - val_acc: 0.9296\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 302s 3ms/step - loss: 0.0241 - acc: 0.9885 - val_loss: 3.7007e-05 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 303s 3ms/step - loss: 0.0184 - acc: 0.9920 - val_loss: 0.0685 - val_acc: 0.9715\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 303s 3ms/step - loss: 0.0473 - acc: 0.9795 - val_loss: 0.0436 - val_acc: 0.9782\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 305s 3ms/step - loss: 0.0176 - acc: 0.9918 - val_loss: 4.9826e-06 - val_acc: 1.0000\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 305s 3ms/step - loss: 0.0158 - acc: 0.9931 - val_loss: 0.0579 - val_acc: 0.9735\n",
      "Train on 112000 samples, validate on 28000 samples\n",
      "Epoch 1/1\n",
      "112000/112000 [==============================] - 305s 3ms/step - loss: 0.0446 - acc: 0.9799 - val_loss: 0.0440 - val_acc: 0.9786\n",
      "[[111063   2117]\n",
      " [   873  25947]]\n",
      "[[140000      0]\n",
      " [     0      0]]\n",
      "[[115873   1924]\n",
      " [  1271  20932]]\n",
      "[[111063   2117]\n",
      " [   873  25947]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Block len 5\n",
      "True positive 0.10320571428571429 216732\n",
      "True negative 0.8837752380952381 1855928\n",
      "False positive 0.005563333333333333 11683\n",
      "False negative 0.007455714285714286 15657\n",
      "Accuracy 0.9869809523809524\n",
      "Precision 0.9488518704988727\n",
      "Recall 0.932625898816209\n",
      "F1 score 1.8191328716847803\n",
      "Educate time 3031.3341426372526\n",
      "Test time 38.33416533470154\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CrossValidation(5 , xnew1, xnew2, xnew3, ynew1, ynew2, ynew3, 300, 2, (5,47), 3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
