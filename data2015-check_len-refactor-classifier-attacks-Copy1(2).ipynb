{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import socket, struct\n",
    "from sklearn.preprocessing import OneHotEncoder, normalize, MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import keras_metrics as km\n",
    "from keras.layers import merge, Input, Dropout, Concatenate,Activation\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SkMetrics(Callback):\n",
    "    def __init__(self,batchsize):\n",
    "        self.batchsize = batchsize\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.confusion = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1s = []\n",
    "        self.kappa = []        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        score = numpy.asarray(self.model.predict(self.validation_data[0],batch_size = self.batchsize))\n",
    "        predict = numpy.round(numpy.asarray(self.model.predict(self.validation_data[0],batch_size = self.batchsize)))\n",
    "        targ = self.validation_data[1]\n",
    "        #targ = targ.flatten()\n",
    "        #predict = predict.flatten()\n",
    "        self.confusion.append(multilabel_confusion_matrix(targ, predict))\n",
    "        #self.precision.append(precision_score(targ, predict))\n",
    "        #self.recall.append(recall_score(targ, predict))\n",
    "        #self.f1s.append(f1_score(targ, predict, average=\"macro\"))\n",
    "        #self.kappa.append(cohen_kappa_score(targ, predict))\n",
    "\n",
    "def getModel(neurons, layers, inputshape, batchsize, numAttacks):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape = (batchsize, inputshape[0], inputshape[1]), return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(neurons, return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(neurons, return_sequences=False, stateful=False))\n",
    "    model.add(Dense(numAttacks, activation = 'sigmoid'))\n",
    "    model.compile(optimizer=\"adam\", \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanet/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/vanet/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./UNSW-NB15_1.csv', header=None)\n",
    "data1a = numpy.asarray(data)\n",
    "\n",
    "data2 = pd.read_csv('./UNSW-NB15_2.csv', header=None)\n",
    "data2a = numpy.asarray(data2)\n",
    "\n",
    "data3 = pd.read_csv('./UNSW-NB15_3.csv', header=None)\n",
    "data3a = numpy.asarray(data3)\n",
    "\n",
    "data = []\n",
    "temp = len(data1a) + len(data2a) + len(data3a)\n",
    "data.append(data1a)\n",
    "data.append(data2a)\n",
    "data.append(data3a)\n",
    "\n",
    "data = numpy.reshape(numpy.asarray(data),(temp, 49))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iptoint(ip):\n",
    "    return struct.unpack(\"!I\",(socket.inet_aton(ip)))[0] \n",
    "\n",
    "def parse(data):\n",
    "    protocols = dict()\n",
    "    protocolsNum = 1\n",
    "    services  = dict()\n",
    "    servicesNum = 1\n",
    "    states = dict()\n",
    "    statesNum = 1\n",
    "    attacksTypes = dict()\n",
    "    attacksNum = 1\n",
    "    numMalicious = 0\n",
    "    numNormal = 0\n",
    "    x = []\n",
    "    y = []\n",
    "    numMaliciousTypes = []\n",
    "    for dataRaw in data:\n",
    "        dataRaw[0] = iptoint(dataRaw[0])\n",
    "        dataRaw[2] = iptoint(dataRaw[2])\n",
    "        #Source IP\n",
    "        if (type(dataRaw[39]) == str):\n",
    "            if (dataRaw[39] !=  \" \"):\n",
    "                dataRaw[39] = int(dataRaw[39],0)\n",
    "            else:\n",
    "                dataRaw[39] = 0\n",
    "        if (type(dataRaw[1]) == str):\n",
    "            if (dataRaw[1] != \"-\"):\n",
    "                dataRaw[1] = int(dataRaw[1],0)\n",
    "            else:\n",
    "                dataRaw[1] = 0\n",
    "        #Dest IP\n",
    "        if (type(dataRaw[3]) == str):\n",
    "            if (dataRaw[3] != \"-\"):\n",
    "                dataRaw[3] = int(dataRaw[3],0)\n",
    "            else:\n",
    "                dataRaw[3] = 0\n",
    "        if (dataRaw[4] in protocols):\n",
    "            dataRaw[4] = protocols.get(dataRaw[4])\n",
    "        else:\n",
    "            protocols.update({dataRaw[4]:protocolsNum})\n",
    "            dataRaw[4] = protocolsNum\n",
    "            protocolsNum += 1\n",
    "\n",
    "        if (type(dataRaw[4])!= int):\n",
    "            print (dataRaw[4])\n",
    " \n",
    "        if (dataRaw[5] in states):\n",
    "            dataRaw[5] = states.get(dataRaw[5])\n",
    "        else:\n",
    "            states.update({dataRaw[5]:statesNum})\n",
    "            dataRaw[5] = statesNum\n",
    "            statesNum += 1\n",
    "        if (type(dataRaw[5])!=int):\n",
    "            print (dataRaw[5])\n",
    "        if (dataRaw[13] in services):\n",
    "            dataRaw[13] = services.get(dataRaw[13])\n",
    "        else:\n",
    "            services.update({dataRaw[13]:servicesNum})\n",
    "            dataRaw[13] = servicesNum\n",
    "            servicesNum += 1\n",
    "        if (type(dataRaw[13])!=int):\n",
    "            print (dataRaw[13])\n",
    "        if (dataRaw[48] == 1):\n",
    "            numMalicious = numMalicious + 1\n",
    "            if (dataRaw[47] in attacksTypes):\n",
    "                y.append(attacksTypes.get(dataRaw[47]))\n",
    "                numMaliciousTypes[attacksTypes.get(dataRaw[47])-1]+=1\n",
    "            else:\n",
    "                attacksTypes.update({dataRaw[47]:attacksNum})\n",
    "                y.append(attacksNum)\n",
    "                attacksNum+=1\n",
    "                numMaliciousTypes.append(1)\n",
    "        else:\n",
    "            numNormal = numNormal + 1\n",
    "            y.append(0)\n",
    "        i = 0\n",
    "        #for data in dataRaw:\n",
    "        #    if (type(data) == str):\n",
    "        #        print(dataRaw,data,i)\n",
    "        #    i = i+1\n",
    "        x.append(dataRaw)\n",
    "    print(\"normal = \",numNormal)\n",
    "    print(\"Malicious = \", numMalicious)\n",
    "    print(\"List TypesMalicousNum\",numMaliciousTypes)\n",
    "    print(\"Attacks: \",attacksTypes)\n",
    "    print(protocols)\n",
    "    print(states)\n",
    "    print(services)\n",
    "    return [x, y, attacksNum]\n",
    "\n",
    "\n",
    "def optimizeParameters(data, labels, scorefunc):\n",
    "    # Выбираем k лучших признаков\n",
    "    selecter = SelectKBest(score_func=scorefunc, k=10)\n",
    "    selecter.fit(data, labels)\n",
    "    \n",
    "    # Какие именно выбрали\n",
    "    params = []\n",
    "    index = []\n",
    "    params = selecter.get_support()\n",
    "    count = 0\n",
    "    \n",
    "    for param in params :\n",
    "        if param == True :\n",
    "            index.append(count)\n",
    "        count += 1\n",
    "    \n",
    "\n",
    "    index_file = open(\"index.txt\" , \"w\")\n",
    "    for i in index : \n",
    "        index_file.write(str(i) + \" \")\n",
    "    index_file.close()\n",
    "\n",
    "    return selecter.transform(data)\n",
    "\n",
    "def drawGraphic(dataset, labels, normalCount, malwareCount):\n",
    "\n",
    "    parametersCount = len(dataset[0])\n",
    "\n",
    "    normalCountList = [0] * parametersCount\n",
    "    malwareCountList = [0] * parametersCount\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        if(i < normalCount):\n",
    "            for j in range(0, parametersCount):\n",
    "                normalCountList[j] += dataset[i][j]\n",
    "        else:\n",
    "            for j in range(0, parametersCount):\n",
    "                malwareCountList[j] += dataset[i][j]\n",
    "\n",
    "    for i in range(0, parametersCount):\n",
    "        normalCountList[i] = normalCountList[i]\n",
    "        malwareCountList[i] = malwareCountList[i] \n",
    "\n",
    "    ts = pd.DataFrame({'normal': normalCountList,\n",
    "                       'malware': malwareCountList})\n",
    "    ts.plot()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "    \n",
    "def ROC(ROC_MODEL , ROC_X, ROC_Y):\n",
    "    probs = ROC_MODEL.predict(ROC_X)\n",
    "    ROC_Y = ROC_Y.flatten()\n",
    "    probs = probs.flatten()\n",
    "    FPr, TPr, threshold = metrics.roc_curve(ROC_Y, probs)\n",
    "    roc_auc = metrics.auc(FPr, TPr)\n",
    "    lw = 2\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(FPr, TPr, lw = lw , label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal =  1867614\n",
      "Malicious =  232389\n",
      "List TypesMalicousNum [33086, 10457, 11446, 153603, 1140, 18856, 131, 1663, 2007]\n",
      "Attacks:  {'Exploits': 1, 'Reconnaissance': 2, 'DoS': 3, 'Generic': 4, 'Shellcode': 5, 'Fuzzers': 6, 'Worms': 7, 'Backdoors': 8, 'Analysis': 9}\n",
      "{'udp': 1, 'arp': 2, 'tcp': 3, 'ospf': 4, 'icmp': 5, 'igmp': 6, 'sctp': 7, 'udt': 8, 'sep': 9, 'sun-nd': 10, 'swipe': 11, 'mobile': 12, 'pim': 13, 'rtp': 14, 'ipnip': 15, 'ip': 16, 'ggp': 17, 'st2': 18, 'egp': 19, 'cbt': 20, 'emcon': 21, 'nvp': 22, 'igp': 23, 'xnet': 24, 'argus': 25, 'bbn-rcc': 26, 'chaos': 27, 'pup': 28, 'hmp': 29, 'mux': 30, 'dcn': 31, 'prm': 32, 'trunk-1': 33, 'xns-idp': 34, 'trunk-2': 35, 'leaf-1': 36, 'leaf-2': 37, 'irtp': 38, 'rdp': 39, 'iso-tp4': 40, 'netblt': 41, 'mfe-nsp': 42, 'merit-inp': 43, '3pc': 44, 'xtp': 45, 'idpr': 46, 'tp++': 47, 'ddp': 48, 'idpr-cmtp': 49, 'ipv6': 50, 'il': 51, 'idrp': 52, 'ipv6-frag': 53, 'sdrp': 54, 'ipv6-route': 55, 'gre': 56, 'rsvp': 57, 'mhrp': 58, 'bna': 59, 'esp': 60, 'i-nlsp': 61, 'narp': 62, 'ipv6-no': 63, 'tlsp': 64, 'skip': 65, 'ipv6-opts': 66, 'any': 67, 'cftp': 68, 'sat-expak': 69, 'kryptolan': 70, 'rvd': 71, 'ippc': 72, 'sat-mon': 73, 'ipcv': 74, 'visa': 75, 'cpnx': 76, 'cphb': 77, 'wsn': 78, 'pvp': 79, 'br-sat-mon': 80, 'wb-mon': 81, 'wb-expak': 82, 'iso-ip': 83, 'secure-vmtp': 84, 'vmtp': 85, 'vines': 86, 'ttp': 87, 'nsfnet-igp': 88, 'dgp': 89, 'tcf': 90, 'eigrp': 91, 'sprite-rpc': 92, 'larp': 93, 'mtp': 94, 'ax.25': 95, 'ipip': 96, 'micp': 97, 'aes-sp3-d': 98, 'encap': 99, 'etherip': 100, 'pri-enc': 101, 'gmtp': 102, 'pnni': 103, 'ifmp': 104, 'aris': 105, 'qnx': 106, 'a/n': 107, 'scps': 108, 'snp': 109, 'ipcomp': 110, 'compaq-peer': 111, 'ipx-n-ip': 112, 'vrrp': 113, 'zero': 114, 'pgm': 115, 'iatp': 116, 'ddx': 117, 'l2tp': 118, 'srp': 119, 'stp': 120, 'smp': 121, 'uti': 122, 'sm': 123, 'ptp': 124, 'fire': 125, 'crtp': 126, 'isis': 127, 'crudp': 128, 'sccopmce': 129, 'sps': 130, 'pipe': 131, 'iplt': 132, 'unas': 133, 'fc': 134, 'ib': 135}\n",
      "{'CON': 1, 'INT': 2, 'FIN': 3, 'URH': 4, 'REQ': 5, 'ECO': 6, 'RST': 7, 'CLO': 8, 'TXD': 9, 'URN': 10, 'no': 11, 'ACC': 12, 'PAR': 13, 'MAS': 14, 'TST': 15, 'ECR': 16}\n",
      "{'dns': 1, '-': 2, 'http': 3, 'smtp': 4, 'ftp-data': 5, 'ftp': 6, 'ssh': 7, 'pop3': 8, 'snmp': 9, 'ssl': 10, 'irc': 11, 'radius': 12, 'dhcp': 13}\n",
      "2100003\n",
      "[0.2711794028719652, 0.021210040436408027, 0.6524071462129994, 9.83321850267745e-08, 0.0, 0.0, 1.2006868117486436e-07, 9.194906523326434e-06, 1.1188787524993125e-05, 0.12156862745098039, 0.11417322834645668, 0.0, 0.0, 0.0, 8.357947830722337e-05, 0.0048290753567918655, 0.00018786398647379298, 0.0001815211472136504, 0.0, 0.0, 0.0, 0.0, 0.043882978723404256, 0.05466666666666666, 0.0, 0.0, 0.0, 0.0, 1.593020328982675e-05, 0.0, 2.8328649663255683e-07, 2.185413140586619e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030303030303030304, 0.09090909090909091, 0.0, 0.030303030303030304, 0.0, 0.0, 0.0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "[x, y, attacksNum] = parse(data)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "newx = numpy.delete(numpy.asarray(x),47,1)\n",
    "newx = numpy.delete(numpy.asarray(newx),47,1)\n",
    "newx = min_max_scaler.fit_transform(newx)\n",
    "\n",
    "newy = y\n",
    "print(len(newx))\n",
    "newx = newx.tolist()\n",
    "print(newx[0])\n",
    "print(newy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "2100000\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "while(len(newx)%100000 != 0):\n",
    "    newx.pop()\n",
    "    newy.pop()\n",
    "newy = np_utils.to_categorical(newy)\n",
    "print(attacksNum)\n",
    "#enc = OneHotEncoder()\n",
    "#newy = enc.fit_transform(y)\n",
    "print(newy[0])\n",
    "print(newy[1])\n",
    "print(newy[2])\n",
    "print(newy[3])\n",
    "print(len(newx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2711794028719652, 0.021210040436408027, 0.6524071462129994, 9.83321850267745e-08, 0.0, 0.0, 1.2006868117486436e-07, 9.194906523326434e-06, 1.1188787524993125e-05, 0.12156862745098039, 0.11417322834645668, 0.0, 0.0, 0.0, 8.357947830722337e-05, 0.0048290753567918655, 0.00018786398647379298, 0.0001815211472136504, 0.0, 0.0, 0.0, 0.0, 0.043882978723404256, 0.05466666666666666, 0.0, 0.0, 0.0, 0.0, 1.593020328982675e-05, 0.0, 2.8328649663255683e-07, 2.185413140586619e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030303030303030304, 0.09090909090909091, 0.0, 0.030303030303030304, 0.0, 0.0, 0.0]\n",
      "47\n",
      "2100000\n"
     ]
    }
   ],
   "source": [
    "print (newx[0])\n",
    "print (len(newx[0]))\n",
    "print (len(newx))\n",
    "\n",
    "def construct_data_for_package_memory_len(package_len, x, y):\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for j in range (len(x) // package_len - 1):\n",
    "        for i in range (package_len):\n",
    "            temp_arr = []\n",
    "            for k in range (package_len):\n",
    "                temp_arr.append(x[j * package_len + i + k])\n",
    "            x_result.append(temp_arr)\n",
    "            y_result.append(y[j * package_len + i + package_len - 1])\n",
    "    while len(x_result) % 10000 != 0:\n",
    "        x_result.pop()\n",
    "        y_result.pop()\n",
    "    #x_result = numpy.asarray(x_result)\n",
    "    #y_result = numpy.asarray(y_result)\n",
    "    for i in range(len(x_result)):\n",
    "        for j in range(len(x_result[i])):\n",
    "            for k in range(len(x_result[i][j])):\n",
    "                if numpy.isnan(x_result[i][j][k]):\n",
    "                    x_result[i][j][k] = 1\n",
    "    return [x_result, y_result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def CrossValidation(lenOfBlock , x, y, neurons, layers, inputshape, epochsIn, batchsize, attacksNum):\n",
    "    cv = KFold(n_splits = lenOfBlock, random_state = None, shuffle = True)\n",
    "        \n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    F1 = 0\n",
    "    labels1 = []\n",
    "    for i in range(attacksNum):\n",
    "        labels1.append(i)\n",
    "    labels1 = np_utils.to_categorical(labels1)\n",
    "    #print(labels1)\n",
    "    cn = multilabel_confusion_matrix(labels1, labels1)\n",
    "    rescn = cn\n",
    "    for j in range (len(cn)):\n",
    "        for g in range (len(cn[0])):\n",
    "            for d in range(len(cn[0][0])):\n",
    "                rescn[j][g][d]=0\n",
    "    class_weights = {0 : 1.,\n",
    "                     1 : 55.,\n",
    "                     2 : 172.,\n",
    "                     3 : 157.,\n",
    "                     4 : 12.,\n",
    "                     5 : 1580.,\n",
    "                     6 : 95.,\n",
    "                     7 : 13000.,\n",
    "                     8 : 1080.,\n",
    "                     9 : 895.}\n",
    "    Educate = 0.0\n",
    "    Test = 0.0\n",
    "    print(\"Neurons: \", neurons)\n",
    "    print(\"Layers: \", layers)\n",
    "    numblock=0\n",
    "    x = numpy.reshape(x,(5,len(x)//5,inputshape[0],47))\n",
    "    y = numpy.reshape(y,(5,len(y)//5,len(y[0])))\n",
    "    for train_index, test_index in cv.split(x[0]):\n",
    "        clf = getModel(neurons, layers, inputshape, batchsize, attacksNum)\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        print(\"Block: \", numblock)\n",
    "        numblock +=1\n",
    "        start_time = time.time()\n",
    "        skmetrics = SkMetrics(batchsize)\n",
    "        for i in range (epochsIn):\n",
    "            history = clf.fit(x[0][train_index], y[0][train_index], callbacks = [skmetrics], epochs = 1, validation_data = (x[0][test_index], y[0][test_index]), batch_size = batchsize, class_weight=class_weights)\n",
    "            history = clf.fit(x[1][train_index], y[1][train_index], callbacks = [skmetrics], epochs = 1, validation_data = (x[1][test_index], y[1][test_index]), batch_size = batchsize, class_weight=class_weights)\n",
    "            history = clf.fit(x[2][train_index], y[2][train_index], callbacks = [skmetrics], epochs = 1, validation_data = (x[2][test_index], y[2][test_index]), batch_size = batchsize, class_weight=class_weights)\n",
    "            history = clf.fit(x[3][train_index], y[3][train_index], callbacks = [skmetrics], epochs = 1, validation_data = (x[3][test_index], y[3][test_index]), batch_size = batchsize, class_weight=class_weights)\n",
    "            history = clf.fit(x[4][train_index], y[4][train_index], callbacks = [skmetrics], epochs = 1, validation_data = (x[4][test_index], y[4][test_index]), batch_size = batchsize, class_weight=class_weights)\n",
    "            i = i\n",
    "        #for k in skmetrics.confusion:\n",
    "        #    print(k)\n",
    "        education_time = time.time() - start_time\n",
    "        try:\n",
    "            # Тестирование\n",
    "            start_time = time.time()\n",
    "            proba = []\n",
    "            for i in range(5):\n",
    "                proba.append(numpy.round(numpy.asarray(clf.predict(x[i][test_index], batch_size = batchsize))))\n",
    "\n",
    "            test_time = time.time() - start_time\n",
    "            # Добавляем значения\n",
    "            Educate += education_time\n",
    "            Test += test_time\n",
    "            y1 = []\n",
    "            for i in range (5):\n",
    "                y1.append(y[i][test_index])\n",
    "            cn = []\n",
    "            print(\"CN: \")\n",
    "            for i in range(5):\n",
    "                cn.append(multilabel_confusion_matrix(y1[i], proba[i]))\n",
    "                print(classification_report(y1[i], proba[i]))\n",
    "                #print(cn[i])\n",
    "                #tn1, fp1, fn1, tp1 = cn[i].ravel()\n",
    "            for mat in range(len(cn)):\n",
    "                for j in range (len(cn[0])):\n",
    "                    for g in range (len(cn[0][0])):\n",
    "                        for d in range(len(cn[0][0][0])):\n",
    "                            rescn[j][g][d]+=cn[mat][j][g][d]\n",
    "            print(rescn)\n",
    "            #tn1, fp1, fn1, tp1 = cn1.ravel()\n",
    "        except ZeroDivisionError:\n",
    "            print(\"ZeroDivisionError\")\n",
    "        #TN += tn1 \n",
    "        #FP += fp1 \n",
    "        #FN += fn1\n",
    "        #TP += tp1 \n",
    "        #F1 += (f1_score(y1, proba , average='macro'))\n",
    "        \n",
    "    #summ = TN + FP + FN + TP\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"Block len \" + str(lenOfBlock))\n",
    "    #print(\"True positive \" + str(TP / summ), TP)\n",
    "    #print(\"True negative \" + str(TN / summ), TN)\n",
    "    #print(\"False positive \" + str(FP / summ), FP)\n",
    "    #print(\"False negative \" + str(FN / summ), FN)\n",
    "   \n",
    "    #print(\"Accuracy \" + str((TP + TN)/(summ)))\n",
    "    #print(\"Precision \" + str(TP / (TP + FP))) # Precision\n",
    "    #print(\"Recall \" + str(TP / (TP + FN))) # Recall\n",
    "\n",
    "    #print(\"F1 score \" + str(F1 / lenOfBlock)) # F1 score\n",
    "    print(\"Educate time \" + str(Educate / lenOfBlock)) # Educate time\n",
    "    print(\"Test time \" + str(Test / lenOfBlock)) # Test time\n",
    "\n",
    "    print(\"\\n\\n\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x, y] = construct_data_for_package_memory_len(5, newx, newy)\n",
    "#x1 = numpy.asarray(x)\n",
    "#y1 = numpy.asarray(y)\n",
    "#CrossValidation(5 , x1, y1, 200, 1, (5,47), 2, 200, attacksNum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x, y] = construct_data_for_package_memory_len(10, newx, newy)\n",
    "#x1 = numpy.asarray(x)\n",
    "#y1 = numpy.asarray(y)\n",
    "#CrossValidation(5 , x1, y1, 200, 1, (10,47), 2, 200, attacksNum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons:  200\n",
      "Layers:  1\n",
      "TRAIN: [     0      1      2 ... 417997 417998 417999] TEST: [     4     13     14 ... 417985 417990 417991]\n",
      "Block:  0\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 193s 578us/step - loss: 16.0396 - acc: 0.6595 - val_loss: 1.2680 - val_acc: 0.8956\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 555us/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 569us/step - loss: 16.6826 - acc: 0.9279 - val_loss: 0.8732 - val_acc: 0.9283\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 557us/step - loss: 31.8637 - acc: 0.7551 - val_loss: 2.3917 - val_acc: 0.1243\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 574us/step - loss: 24.8799 - acc: 0.2285 - val_loss: 1.5951 - val_acc: 0.4054\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 572us/step - loss: 12.5595 - acc: 0.9078 - val_loss: 0.1870 - val_acc: 0.9561\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 181s 542us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 180s 540us/step - loss: 10.4857 - acc: 0.9491 - val_loss: 0.1555 - val_acc: 0.9624\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 193s 577us/step - loss: 19.5854 - acc: 0.8942 - val_loss: 0.2644 - val_acc: 0.9279\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 187s 559us/step - loss: 15.7380 - acc: 0.9343 - val_loss: 0.2440 - val_acc: 0.9396\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 183s 548us/step - loss: 10.1554 - acc: 0.9596 - val_loss: 0.1597 - val_acc: 0.9619\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 571us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 181s 540us/step - loss: 8.7691 - acc: 0.9640 - val_loss: 0.1079 - val_acc: 0.9697\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 573us/step - loss: 17.1760 - acc: 0.9258 - val_loss: 0.2540 - val_acc: 0.9276\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 570us/step - loss: 13.9723 - acc: 0.9412 - val_loss: 0.1986 - val_acc: 0.9451\n",
      "CN: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanet/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92     79023\n",
      "           1       0.83      0.11      0.19      1078\n",
      "           2       0.53      0.35      0.42       380\n",
      "           3       0.14      0.39      0.20       230\n",
      "           4       1.00      0.00      0.00      1526\n",
      "           5       0.08      0.50      0.14        58\n",
      "           6       0.32      0.45      0.37      1087\n",
      "           7       0.01      0.17      0.02         6\n",
      "           8       0.14      0.86      0.23       101\n",
      "           9       0.17      0.81      0.28       111\n",
      "\n",
      "   micro avg       0.96      0.82      0.89     83600\n",
      "   macro avg       0.42      0.45      0.28     83600\n",
      "weighted avg       0.98      0.82      0.88     83600\n",
      " samples avg       0.82      0.82      0.82     83600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanet/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vanet/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     83600\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       1.00      0.92      0.96     83600\n",
      "   macro avg       0.10      0.09      0.10     83600\n",
      "weighted avg       1.00      0.92      0.96     83600\n",
      " samples avg       0.92      0.92      0.92     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     77608\n",
      "           1       0.72      0.19      0.30      1025\n",
      "           2       0.76      0.26      0.39       366\n",
      "           3       0.40      0.83      0.54       528\n",
      "           4       1.00      0.72      0.84      3495\n",
      "           5       0.09      0.82      0.15        39\n",
      "           6       0.44      0.55      0.49       419\n",
      "           7       0.02      1.00      0.03         3\n",
      "           8       0.04      0.82      0.07        51\n",
      "           9       0.06      0.71      0.11        66\n",
      "\n",
      "   micro avg       0.96      0.89      0.92     83600\n",
      "   macro avg       0.45      0.68      0.39     83600\n",
      "weighted avg       0.99      0.89      0.93     83600\n",
      " samples avg       0.88      0.89      0.88     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     66269\n",
      "           1       0.74      0.18      0.29      2675\n",
      "           2       0.71      0.29      0.41       733\n",
      "           3       0.28      0.77      0.41       867\n",
      "           4       1.00      0.92      0.96     11576\n",
      "           5       0.06      0.71      0.11        72\n",
      "           6       0.39      0.44      0.41      1185\n",
      "           7       0.01      0.57      0.02         7\n",
      "           8       0.03      0.71      0.05        86\n",
      "           9       0.05      0.72      0.10       130\n",
      "\n",
      "   micro avg       0.90      0.88      0.89     83600\n",
      "   macro avg       0.43      0.62      0.37     83600\n",
      "weighted avg       0.97      0.88      0.91     83600\n",
      " samples avg       0.87      0.88      0.87     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     65134\n",
      "           1       0.71      0.22      0.33      1864\n",
      "           2       0.66      0.24      0.35       652\n",
      "           3       0.34      0.70      0.46       649\n",
      "           4       1.00      0.94      0.97     13983\n",
      "           5       0.06      0.66      0.11        73\n",
      "           6       0.42      0.42      0.42      1042\n",
      "           7       0.01      0.56      0.03         9\n",
      "           8       0.05      0.80      0.09        75\n",
      "           9       0.05      0.39      0.09       119\n",
      "\n",
      "   micro avg       0.94      0.89      0.91     83600\n",
      "   macro avg       0.43      0.58      0.38     83600\n",
      "weighted avg       0.98      0.89      0.93     83600\n",
      " samples avg       0.88      0.89      0.89     83600\n",
      "\n",
      "[[[ 46366      0]\n",
      "  [ 36420 335214]]\n",
      "\n",
      " [[410920    438]\n",
      "  [  5439   1203]]\n",
      "\n",
      " [[415549    320]\n",
      "  [  1534    597]]\n",
      "\n",
      " [[411937   3789]\n",
      "  [   621   1653]]\n",
      "\n",
      " [[387374     46]\n",
      "  [  4253  26327]]\n",
      "\n",
      " [[415460   2298]\n",
      "  [    82    160]]\n",
      "\n",
      " [[411466   2801]\n",
      "  [  2043   1690]]\n",
      "\n",
      " [[417048    927]\n",
      "  [    12     13]]\n",
      "\n",
      " [[412637   5050]\n",
      "  [    63    250]]\n",
      "\n",
      " [[413847   3727]\n",
      "  [   149    277]]]\n",
      "TRAIN: [     0      2      4 ... 417995 417997 417998] TEST: [     1      3      7 ... 417993 417996 417999]\n",
      "Block:  1\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 573us/step - loss: 16.5899 - acc: 0.7417 - val_loss: 0.9238 - val_acc: 0.9332\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 569us/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 182s 544us/step - loss: 14.3013 - acc: 0.7357 - val_loss: 0.6812 - val_acc: 0.8902\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 568us/step - loss: 23.1221 - acc: 0.8838 - val_loss: 0.3847 - val_acc: 0.9182\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 187s 560us/step - loss: 16.9334 - acc: 0.9274 - val_loss: 0.2809 - val_acc: 0.9313\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 188s 564us/step - loss: 12.1480 - acc: 0.9384 - val_loss: 0.1880 - val_acc: 0.9610\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 574us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 185s 554us/step - loss: 9.4774 - acc: 0.9603 - val_loss: 0.1482 - val_acc: 0.9609\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 194s 580us/step - loss: 18.5280 - acc: 0.9185 - val_loss: 0.2759 - val_acc: 0.9238\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 204s 610us/step - loss: 14.4461 - acc: 0.9359 - val_loss: 0.2380 - val_acc: 0.9395\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 556us/step - loss: 10.6695 - acc: 0.9564 - val_loss: 0.2228 - val_acc: 0.9470\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 189s 566us/step - loss: 0.0045 - acc: 0.9999 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 181s 540us/step - loss: 8.3919 - acc: 0.9630 - val_loss: 0.1119 - val_acc: 0.9691\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 569us/step - loss: 17.6388 - acc: 0.9214 - val_loss: 0.2578 - val_acc: 0.9250\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 569us/step - loss: 13.7504 - acc: 0.9396 - val_loss: 0.2213 - val_acc: 0.9450\n",
      "CN: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94     79203\n",
      "           1       0.57      0.02      0.04      1083\n",
      "           2       0.22      0.35      0.27       332\n",
      "           3       0.13      0.45      0.21       224\n",
      "           4       0.00      0.00      0.00      1451\n",
      "           5       0.02      0.24      0.04        38\n",
      "           6       0.34      0.05      0.08      1059\n",
      "           7       0.01      0.67      0.01         3\n",
      "           8       0.13      0.89      0.23       113\n",
      "           9       0.14      0.86      0.24        94\n",
      "\n",
      "   micro avg       0.96      0.85      0.90     83600\n",
      "   macro avg       0.26      0.44      0.21     83600\n",
      "weighted avg       0.96      0.85      0.90     83600\n",
      " samples avg       0.85      0.85      0.85     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97     83600\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       1.00      0.93      0.97     83600\n",
      "   macro avg       0.10      0.09      0.10     83600\n",
      "weighted avg       1.00      0.93      0.97     83600\n",
      " samples avg       0.93      0.93      0.93     83600\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96     77610\n",
      "           1       0.55      0.05      0.10      1037\n",
      "           2       0.51      0.51      0.51       358\n",
      "           3       0.36      0.79      0.49       497\n",
      "           4       1.00      0.58      0.74      3515\n",
      "           5       0.05      0.76      0.10        29\n",
      "           6       0.44      0.25      0.32       410\n",
      "           7       0.01      0.60      0.03         5\n",
      "           8       0.05      0.87      0.10        67\n",
      "           9       0.05      0.75      0.10        72\n",
      "\n",
      "   micro avg       0.95      0.90      0.92     83600\n",
      "   macro avg       0.40      0.61      0.34     83600\n",
      "weighted avg       0.98      0.90      0.93     83600\n",
      " samples avg       0.89      0.90      0.89     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93     65997\n",
      "           1       0.60      0.04      0.08      2678\n",
      "           2       0.46      0.57      0.51       680\n",
      "           3       0.28      0.73      0.41       893\n",
      "           4       1.00      0.91      0.95     11879\n",
      "           5       0.06      0.69      0.11        80\n",
      "           6       0.46      0.29      0.35      1178\n",
      "           7       0.02      0.88      0.03         8\n",
      "           8       0.02      0.67      0.04        79\n",
      "           9       0.05      0.80      0.09       128\n",
      "\n",
      "   micro avg       0.90      0.84      0.86     83600\n",
      "   macro avg       0.39      0.64      0.35     83600\n",
      "weighted avg       0.96      0.84      0.89     83600\n",
      " samples avg       0.83      0.84      0.83     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.94     65148\n",
      "           1       0.45      0.04      0.07      1818\n",
      "           2       0.44      0.56      0.49       694\n",
      "           3       0.36      0.70      0.48       660\n",
      "           4       1.00      0.92      0.96     13919\n",
      "           5       0.05      0.72      0.09        67\n",
      "           6       0.44      0.23      0.30      1112\n",
      "           7       0.03      0.69      0.05        13\n",
      "           8       0.04      0.69      0.07        71\n",
      "           9       0.04      0.44      0.07        98\n",
      "\n",
      "   micro avg       0.93      0.86      0.89     83600\n",
      "   macro avg       0.38      0.59      0.35     83600\n",
      "weighted avg       0.97      0.86      0.90     83600\n",
      " samples avg       0.85      0.86      0.85     83600\n",
      "\n",
      "[[[ 92808      0]\n",
      "  [ 72641 670551]]\n",
      "\n",
      " [[822083    659]\n",
      "  [ 11800   1458]]\n",
      "\n",
      " [[829939   1866]\n",
      "  [  2524   1671]]\n",
      "\n",
      " [[823840   7612]\n",
      "  [  1288   3260]]\n",
      "\n",
      " [[774606     50]\n",
      "  [  9314  52030]]\n",
      "\n",
      " [[830651   4893]\n",
      "  [   162    294]]\n",
      "\n",
      " [[824760   3748]\n",
      "  [  5061   2431]]\n",
      "\n",
      " [[833767   2179]\n",
      "  [    20     34]]\n",
      "\n",
      " [[825124  10233]\n",
      "  [   132    511]]\n",
      "\n",
      " [[826974   8208]\n",
      "  [   261    557]]]\n",
      "TRAIN: [     0      1      2 ... 417997 417998 417999] TEST: [     5      6     11 ... 417983 417986 417988]\n",
      "Block:  2\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 575us/step - loss: 16.0146 - acc: 0.7025 - val_loss: 0.5658 - val_acc: 0.9357\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 571us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 184s 549us/step - loss: 16.7907 - acc: 0.9241 - val_loss: 2.0964 - val_acc: 0.9263\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 180s 538us/step - loss: 31.7353 - acc: 0.1068 - val_loss: 2.2637 - val_acc: 0.1355\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 574us/step - loss: 26.6916 - acc: 0.1674 - val_loss: 2.0707 - val_acc: 0.1681\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 188s 562us/step - loss: 13.0693 - acc: 0.8775 - val_loss: 0.3822 - val_acc: 0.9090\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 558us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 568us/step - loss: 11.2010 - acc: 0.9340 - val_loss: 0.1789 - val_acc: 0.9531\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 183s 546us/step - loss: 20.1326 - acc: 0.9102 - val_loss: 0.2986 - val_acc: 0.9226\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 571us/step - loss: 16.5851 - acc: 0.9276 - val_loss: 0.2359 - val_acc: 0.9351\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 570us/step - loss: 10.6701 - acc: 0.9554 - val_loss: 0.1437 - val_acc: 0.9624\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 187s 558us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 193s 577us/step - loss: 9.2042 - acc: 0.9582 - val_loss: 0.1216 - val_acc: 0.9654\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 555us/step - loss: 18.3946 - acc: 0.9180 - val_loss: 0.2469 - val_acc: 0.9262\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 570us/step - loss: 14.8972 - acc: 0.9353 - val_loss: 0.2091 - val_acc: 0.9416\n",
      "CN: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     79120\n",
      "           1       0.91      0.28      0.43      1137\n",
      "           2       0.32      0.69      0.44       350\n",
      "           3       0.14      0.48      0.22       216\n",
      "           4       1.00      0.01      0.02      1513\n",
      "           5       0.03      0.65      0.06        52\n",
      "           6       0.22      0.13      0.17       994\n",
      "           7       0.00      0.17      0.01         6\n",
      "           8       0.14      0.87      0.24       115\n",
      "           9       0.12      0.67      0.20        97\n",
      "\n",
      "   micro avg       0.95      0.93      0.94     83600\n",
      "   macro avg       0.39      0.49      0.28     83600\n",
      "weighted avg       0.98      0.93      0.94     83600\n",
      " samples avg       0.93      0.93      0.93     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     83600\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     83600\n",
      "   macro avg       0.10      0.10      0.10     83600\n",
      "weighted avg       1.00      1.00      1.00     83600\n",
      " samples avg       1.00      1.00      1.00     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     77442\n",
      "           1       0.94      0.06      0.11      1068\n",
      "           2       0.42      0.35      0.38       299\n",
      "           3       0.39      0.83      0.53       533\n",
      "           4       1.00      0.87      0.93      3612\n",
      "           5       0.03      0.76      0.05        25\n",
      "           6       0.45      0.34      0.39       485\n",
      "           7       0.02      0.17      0.04         6\n",
      "           8       0.05      0.91      0.09        58\n",
      "           9       0.05      0.64      0.09        72\n",
      "\n",
      "   micro avg       0.96      0.95      0.95     83600\n",
      "   macro avg       0.43      0.59      0.36     83600\n",
      "weighted avg       0.99      0.95      0.96     83600\n",
      " samples avg       0.95      0.95      0.95     83600\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     66431\n",
      "           1       0.86      0.05      0.10      2578\n",
      "           2       0.41      0.42      0.41       723\n",
      "           3       0.27      0.73      0.40       854\n",
      "           4       1.00      0.95      0.97     11562\n",
      "           5       0.04      0.83      0.07        76\n",
      "           6       0.40      0.30      0.34      1181\n",
      "           7       0.01      0.25      0.02         4\n",
      "           8       0.03      0.73      0.05        83\n",
      "           9       0.04      0.69      0.08       108\n",
      "\n",
      "   micro avg       0.90      0.89      0.90     83600\n",
      "   macro avg       0.41      0.59      0.34     83600\n",
      "weighted avg       0.97      0.89      0.92     83600\n",
      " samples avg       0.89      0.89      0.89     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     65064\n",
      "           1       0.91      0.07      0.13      1838\n",
      "           2       0.39      0.38      0.39       696\n",
      "           3       0.34      0.71      0.46       650\n",
      "           4       1.00      0.96      0.98     14006\n",
      "           5       0.03      0.85      0.06        55\n",
      "           6       0.42      0.26      0.32      1094\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.05      0.79      0.09        84\n",
      "           9       0.04      0.41      0.08       108\n",
      "\n",
      "   micro avg       0.93      0.91      0.92     83600\n",
      "   macro avg       0.42      0.54      0.35     83600\n",
      "weighted avg       0.98      0.91      0.93     83600\n",
      " samples avg       0.90      0.91      0.90     83600\n",
      "\n",
      "[[[ 139150       1]\n",
      "  [  85418 1029431]]\n",
      "\n",
      " [[1233391     730]\n",
      "  [  17775    2104]]\n",
      "\n",
      " [[1244367    3370]\n",
      "  [   3677    2586]]\n",
      "\n",
      " [[1235713   11486]\n",
      "  [   1910    4891]]\n",
      "\n",
      " [[1161849     114]\n",
      "  [  12430   79607]]\n",
      "\n",
      " [[1243447    9889]\n",
      "  [    207     457]]\n",
      "\n",
      " [[1237417    5337]\n",
      "  [   7883    3363]]\n",
      "\n",
      " [[1251240    2685]\n",
      "  [     38      37]]\n",
      "\n",
      " [[1237589   15428]\n",
      "  [    192     791]]\n",
      "\n",
      " [[1240550   12247]\n",
      "  [    416     787]]]\n",
      "TRAIN: [     1      2      3 ... 417996 417998 417999] TEST: [     0      8      9 ... 417989 417995 417997]\n",
      "Block:  3\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 570us/step - loss: 16.4116 - acc: 0.6850 - val_loss: 1.7112 - val_acc: 0.8304\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 567us/step - loss: 0.0711 - acc: 0.9999 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 569us/step - loss: 14.2772 - acc: 0.7055 - val_loss: 0.7924 - val_acc: 0.6112\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 184s 550us/step - loss: 22.2065 - acc: 0.8075 - val_loss: 0.5322 - val_acc: 0.8441\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 569us/step - loss: 16.7700 - acc: 0.9145 - val_loss: 0.3856 - val_acc: 0.9156\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 189s 565us/step - loss: 11.3287 - acc: 0.9446 - val_loss: 0.1912 - val_acc: 0.9539\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 187s 561us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 575us/step - loss: 9.6306 - acc: 0.9578 - val_loss: 0.1344 - val_acc: 0.9628\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 555us/step - loss: 18.5821 - acc: 0.9129 - val_loss: 0.3493 - val_acc: 0.9104\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 193s 577us/step - loss: 15.1401 - acc: 0.9333 - val_loss: 0.2692 - val_acc: 0.9277\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 192s 573us/step - loss: 10.4280 - acc: 0.9584 - val_loss: 0.1216 - val_acc: 0.9670\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 557us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 573us/step - loss: 8.2852 - acc: 0.9666 - val_loss: 0.1183 - val_acc: 0.9648\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 555us/step - loss: 17.0009 - acc: 0.9251 - val_loss: 0.2541 - val_acc: 0.9283\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 179s 535us/step - loss: 13.9306 - acc: 0.9369 - val_loss: 0.2037 - val_acc: 0.9420\n",
      "CN: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     79206\n",
      "           1       0.44      0.02      0.04      1068\n",
      "           2       0.36      0.39      0.37       358\n",
      "           3       0.17      0.49      0.25       242\n",
      "           4       1.00      0.27      0.43      1526\n",
      "           5       0.03      0.50      0.06        36\n",
      "           6       0.28      0.51      0.36       958\n",
      "           7       0.01      0.83      0.01         6\n",
      "           8       0.12      0.84      0.22       102\n",
      "           9       0.13      0.86      0.23        98\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     83600\n",
      "   macro avg       0.35      0.57      0.30     83600\n",
      "weighted avg       0.98      0.95      0.95     83600\n",
      " samples avg       0.94      0.95      0.94     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     83600\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     83600\n",
      "   macro avg       0.10      0.10      0.10     83600\n",
      "weighted avg       1.00      1.00      1.00     83600\n",
      " samples avg       1.00      1.00      1.00     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     77551\n",
      "           1       0.50      0.10      0.17      1020\n",
      "           2       0.55      0.44      0.49       323\n",
      "           3       0.38      0.82      0.52       533\n",
      "           4       1.00      0.87      0.93      3569\n",
      "           5       0.05      0.75      0.10        40\n",
      "           6       0.27      0.61      0.37       427\n",
      "           7       0.00      1.00      0.01         1\n",
      "           8       0.05      0.86      0.09        65\n",
      "           9       0.05      0.75      0.09        71\n",
      "\n",
      "   micro avg       0.94      0.96      0.95     83600\n",
      "   macro avg       0.39      0.72      0.38     83600\n",
      "weighted avg       0.98      0.96      0.97     83600\n",
      " samples avg       0.95      0.96      0.95     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95     66210\n",
      "           1       0.66      0.10      0.18      2554\n",
      "           2       0.45      0.45      0.45       715\n",
      "           3       0.28      0.75      0.41       857\n",
      "           4       0.99      0.96      0.98     11805\n",
      "           5       0.05      0.83      0.09        78\n",
      "           6       0.33      0.65      0.44      1181\n",
      "           7       0.01      1.00      0.02         9\n",
      "           8       0.02      0.71      0.04        72\n",
      "           9       0.04      0.82      0.08       119\n",
      "\n",
      "   micro avg       0.88      0.87      0.87     83600\n",
      "   macro avg       0.38      0.72      0.36     83600\n",
      "weighted avg       0.96      0.87      0.91     83600\n",
      " samples avg       0.86      0.87      0.86     83600\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95     65202\n",
      "           1       0.64      0.11      0.19      1872\n",
      "           2       0.40      0.43      0.41       657\n",
      "           3       0.34      0.68      0.46       679\n",
      "           4       1.00      0.97      0.99     13817\n",
      "           5       0.05      0.84      0.09        82\n",
      "           6       0.33      0.62      0.43      1089\n",
      "           7       0.01      0.80      0.01         5\n",
      "           8       0.05      0.68      0.09        95\n",
      "           9       0.04      0.49      0.07       102\n",
      "\n",
      "   micro avg       0.91      0.89      0.90     83600\n",
      "   macro avg       0.39      0.65      0.37     83600\n",
      "weighted avg       0.97      0.89      0.92     83600\n",
      " samples avg       0.88      0.89      0.89     83600\n",
      "\n",
      "[[[ 185380       2]\n",
      "  [ 101423 1385195]]\n",
      "\n",
      " [[1644482    1125]\n",
      "  [  23686    2707]]\n",
      "\n",
      " [[1659129    4555]\n",
      "  [   4842    3474]]\n",
      "\n",
      " [[1647569   15319]\n",
      "  [   2564    6548]]\n",
      "\n",
      " [[1549058     188]\n",
      "  [  14813  107941]]\n",
      "\n",
      " [[1657490   13610]\n",
      "  [    261     639]]\n",
      "\n",
      " [[1646898   10201]\n",
      "  [   9358    5543]]\n",
      "\n",
      " [[1666496    5408]\n",
      "  [     40      56]]\n",
      "\n",
      " [[1650058   20625]\n",
      "  [    268    1049]]\n",
      "\n",
      " [[1653243   17164]\n",
      "  [    522    1071]]]\n",
      "TRAIN: [     0      1      3 ... 417996 417997 417999] TEST: [     2     15     24 ... 417976 417994 417998]\n",
      "Block:  4\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 193s 577us/step - loss: 16.6850 - acc: 0.6968 - val_loss: 1.8117 - val_acc: 0.5748\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 187s 559us/step - loss: 0.1063 - acc: 0.9983 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 188s 562us/step - loss: 15.6279 - acc: 0.7650 - val_loss: 0.9421 - val_acc: 0.6021\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 572us/step - loss: 24.8937 - acc: 0.4495 - val_loss: 0.5043 - val_acc: 0.8881\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 184s 549us/step - loss: 18.3848 - acc: 0.9123 - val_loss: 0.2516 - val_acc: 0.9410\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 567us/step - loss: 11.7673 - acc: 0.9528 - val_loss: 0.1726 - val_acc: 0.9642\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 188s 562us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 185s 554us/step - loss: 9.3341 - acc: 0.9621 - val_loss: 0.1347 - val_acc: 0.9681\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 572us/step - loss: 18.3045 - acc: 0.9127 - val_loss: 0.2524 - val_acc: 0.9322\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 184s 549us/step - loss: 15.6444 - acc: 0.9284 - val_loss: 0.2257 - val_acc: 0.9428\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 190s 567us/step - loss: 11.1239 - acc: 0.9532 - val_loss: 0.1607 - val_acc: 0.9629\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 189s 564us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 182s 544us/step - loss: 8.4682 - acc: 0.9672 - val_loss: 0.1261 - val_acc: 0.9679\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 191s 570us/step - loss: 17.4769 - acc: 0.9198 - val_loss: 0.2417 - val_acc: 0.9303\n",
      "Train on 334400 samples, validate on 83600 samples\n",
      "Epoch 1/1\n",
      "334400/334400 [==============================] - 186s 556us/step - loss: 15.1098 - acc: 0.9307 - val_loss: 0.2256 - val_acc: 0.9420\n",
      "CN: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     79233\n",
      "           1       0.50      0.51      0.51      1043\n",
      "           2       0.36      0.34      0.35       339\n",
      "           3       0.17      0.51      0.26       255\n",
      "           4       0.59      0.02      0.03      1506\n",
      "           5       0.03      0.41      0.05        39\n",
      "           6       0.55      0.15      0.24       953\n",
      "           7       0.00      0.67      0.00         3\n",
      "           8       0.12      0.87      0.21       103\n",
      "           9       0.13      0.81      0.23       126\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     83600\n",
      "   macro avg       0.35      0.53      0.29     83600\n",
      "weighted avg       0.97      0.95      0.95     83600\n",
      " samples avg       0.94      0.95      0.94     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     83600\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     83600\n",
      "   macro avg       0.10      0.10      0.10     83600\n",
      "weighted avg       1.00      1.00      1.00     83600\n",
      " samples avg       1.00      1.00      1.00     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     77687\n",
      "           1       0.48      0.58      0.53      1075\n",
      "           2       0.55      0.22      0.31       305\n",
      "           3       0.37      0.83      0.51       508\n",
      "           4       1.00      0.82      0.90      3429\n",
      "           5       0.03      0.47      0.06        32\n",
      "           6       0.54      0.34      0.42       418\n",
      "           7       0.01      0.80      0.02         5\n",
      "           8       0.05      0.81      0.10        72\n",
      "           9       0.04      0.70      0.08        69\n",
      "\n",
      "   micro avg       0.95      0.97      0.96     83600\n",
      "   macro avg       0.41      0.65      0.39     83600\n",
      "weighted avg       0.98      0.97      0.97     83600\n",
      " samples avg       0.96      0.97      0.96     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     66176\n",
      "           1       0.60      0.57      0.58      2499\n",
      "           2       0.50      0.27      0.35       737\n",
      "           3       0.29      0.74      0.42       870\n",
      "           4       1.00      0.95      0.97     11852\n",
      "           5       0.04      0.52      0.07        83\n",
      "           6       0.54      0.34      0.42      1174\n",
      "           7       0.02      0.72      0.04        18\n",
      "           8       0.03      0.77      0.05        81\n",
      "           9       0.04      0.86      0.08       110\n",
      "\n",
      "   micro avg       0.89      0.93      0.91     83600\n",
      "   macro avg       0.41      0.67      0.40     83600\n",
      "weighted avg       0.97      0.93      0.94     83600\n",
      " samples avg       0.91      0.93      0.92     83600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     65147\n",
      "           1       0.51      0.53      0.52      1784\n",
      "           2       0.49      0.22      0.31       653\n",
      "           3       0.34      0.68      0.45       632\n",
      "           4       1.00      0.96      0.98     14012\n",
      "           5       0.04      0.48      0.07        77\n",
      "           6       0.54      0.29      0.38      1102\n",
      "           7       0.01      0.71      0.01         7\n",
      "           8       0.05      0.77      0.09        79\n",
      "           9       0.05      0.63      0.09       107\n",
      "\n",
      "   micro avg       0.92      0.94      0.93     83600\n",
      "   macro avg       0.40      0.62      0.39     83600\n",
      "weighted avg       0.97      0.94      0.95     83600\n",
      " samples avg       0.93      0.94      0.93     83600\n",
      "\n",
      "[[[ 231536       3]\n",
      "  [ 108662 1749799]]\n",
      "\n",
      " [[2053020    4186]\n",
      "  [  26577    6217]]\n",
      "\n",
      " [[2074483    5167]\n",
      "  [   6347    4003]]\n",
      "\n",
      " [[2059514   19109]\n",
      "  [   3201    8176]]\n",
      "\n",
      " [[1936168     279]\n",
      "  [  18031  135522]]\n",
      "\n",
      " [[2072163   16706]\n",
      "  [    381     750]]\n",
      "\n",
      " [[2060389   11063]\n",
      "  [  11989    6559]]\n",
      "\n",
      " [[2081755    8116]\n",
      "  [     49      80]]\n",
      "\n",
      " [[2062625   25723]\n",
      "  [    332    1320]]\n",
      "\n",
      " [[2065715   22280]\n",
      "  [    622    1383]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Block len 5\n",
      "Educate time 3327.8849324703215\n",
      "Test time 77.71481051445008\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "[x, y] = construct_data_for_package_memory_len(5, newx, newy)\n",
    "x1 = numpy.asarray(x)\n",
    "y1 = numpy.asarray(y)\n",
    "CrossValidation(5 , x1, y1, 200, 1, (5,47), 3, 400, attacksNum)\n",
    "\n",
    "\n",
    "\n",
    "#[x, y] = construct_data_for_package_memory_len(20, newx, newy)\n",
    "#x1 = numpy.asarray(x)\n",
    "#y1 = numpy.asarray(y)\n",
    "#CrossValidation(5 , x1, y1, 200, 1, (20,47), 2, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[x, y] = construct_data_for_package_memory_len(3, newx, newy)\n",
    "x1 = numpy.asarray(x)\n",
    "y1 = numpy.asarray(y)\n",
    "CrossValidation(5 , x1, y1, 200, 1, (3,47), 2, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x, y] = construct_data_for_package_memory_len(7, newx, newy)\n",
    "x1 = numpy.asarray(x)\n",
    "y1 = numpy.asarray(y)\n",
    "CrossValidation(5 , x1, y1, 200, 1, (7,47), 2, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
